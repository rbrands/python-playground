{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Working with Azure Blob Storage and Machine Learning\n",
    "## Wine Quality Prediction Pipeline\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "This notebook demonstrates how to integrate **Azure Blob Storage** into a typical data science workflow. We'll build a complete pipeline from data storage to machine learning predictions.\n",
    "\n",
    "**What we'll learn:**\n",
    "- Connect to Azure Blob Storage from Python\n",
    "- Create containers and upload datasets\n",
    "- Download and process data from the cloud\n",
    "- Build a regression model (predicting continuous values)\n",
    "- Store results and models back to Azure\n",
    "- Best practices for cloud data management\n",
    "\n",
    "### Why Azure Blob Storage?\n",
    "\n",
    "**Benefits for Data Scientists:**\n",
    "- ðŸ“¦ Store large datasets (GB/TB) without cluttering Git repositories\n",
    "- ðŸ”„ Share data easily across team members\n",
    "- ðŸ’° Cost-effective storage (pay only for what you use)\n",
    "- ðŸŒ Access data from anywhere (local, Codespace, Azure ML)\n",
    "- ðŸ“Š Separate data from code (better organization)\n",
    "- ðŸ” Secure with access controls and encryption\n",
    "\n",
    "**Use Cases:**\n",
    "- Raw datasets too large for Git (> 100 MB)\n",
    "- Processed data and feature engineering results\n",
    "- Trained models and model artifacts\n",
    "- Experiment results and predictions\n",
    "- Shared datasets for team collaboration\n",
    "\n",
    "### The Dataset: Wine Quality\n",
    "\n",
    "We'll use the **Wine Quality Dataset** from the UCI Machine Learning Repository.\n",
    "\n",
    "**Dataset Details:**\n",
    "- Source: Portuguese \"Vinho Verde\" wines\n",
    "- Samples: ~6,500 wines (red and white variants)\n",
    "- Features: 11 physicochemical properties\n",
    "  - Fixed acidity, volatile acidity, citric acid\n",
    "  - Residual sugar, chlorides\n",
    "  - Free and total sulfur dioxide\n",
    "  - Density, pH\n",
    "  - Sulphates, alcohol content\n",
    "- Target: Quality score (0-10, based on sensory data)\n",
    "\n",
    "**ML Task:** **Regression** (predicting a continuous quality score)\n",
    "- This is different from Iris (classification with categories)\n",
    "- We'll predict numeric values instead of classes\n",
    "\n",
    "### Technical Setup\n",
    "\n",
    "**Requirements:**\n",
    "- Azure Storage Account (created: `rbrandsdataplayground`)\n",
    "- Connection String (stored as Codespace Secret AZURE_STORAGE_CONNECTION_STRING)\n",
    "- Python packages: `azure-storage-blob`, `pandas`, `scikit-learn`, `matplotlib`\n",
    "\n",
    "**Notebook Structure:**\n",
    "1. Setup & Connection to Azure Blob Storage\n",
    "2. Create Container & Upload Dataset\n",
    "3. Download & Explore Data\n",
    "4. Data Analysis & Visualization\n",
    "5. Machine Learning - Regression Model\n",
    "6. Save Results & Model to Blob Storage\n",
    "7. Best Practices & Cleanup\n",
    "\n",
    "Let's get started! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup and Establishing Connection \n",
    "\n",
    "In this first step, we set up the connection to Azure Blob Storage.\n",
    "We use the `azure-storage-blob` library that we've already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Imports\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file (if it exists)\n",
    "# If you're using Codespace Secrets, this step is optional\n",
    "load_dotenv()\n",
    "\n",
    "# %%\n",
    "# Load connection string from environment variable\n",
    "connection_string = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
    "\n",
    "# Verify that connection string is available\n",
    "if not connection_string:\n",
    "    raise ValueError(\n",
    "        \"Azure connection string not found! Please set:\\n\"\n",
    "        \"AZURE_STORAGE_CONNECTION_STRING\\n\"\n",
    "        \"as a Codespace Secret or in a .env file\"\n",
    "    )\n",
    "\n",
    "# Create BlobServiceClient\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Extract storage account name from connection string for display\n",
    "storage_account_name = connection_string.split('AccountName=')[1].split(';')[0]\n",
    "print(f\"âœ“ Connection established to Storage Account '{storage_account_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### What's happening here?\n",
    "\n",
    "1. **Imports**: We import the necessary classes from the Azure SDK\n",
    "2. **Connection String**: We load the complete connection string from the environment variable\n",
    "   - This works with Codespace Secrets (recommended)\n",
    "   - Also works with a `.env` file for local development\n",
    "3. **Validation**: We check if the connection string is available before proceeding\n",
    "4. **BlobServiceClient**: This is our main object for all operations with Azure Blob Storage\n",
    "\n",
    "**Why use the connection string directly?**\n",
    "- Simpler: Only one environment variable instead of two\n",
    "- Standard practice: Azure documentation often uses connection strings\n",
    "- Flexible: The connection string contains all necessary information\n",
    "\n",
    "**Security Note**: The connection string contains sensitive data - never commit it to your repository!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Test the connection by listing all containers\n",
    "print(\"\\nExisting containers:\")\n",
    "try:\n",
    "    containers = blob_service_client.list_containers()\n",
    "    container_list = [container.name for container in containers]\n",
    "\n",
    "    if container_list:\n",
    "        for container_name in container_list:\n",
    "            print(f\"  - {container_name}\")\n",
    "    else:\n",
    "        print(\"  (No containers yet)\")\n",
    "\n",
    "    print(f\"\\nâœ“ Connection successfully tested!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Connection error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. Creating and Managing Containers\n",
    "\n",
    "Containers in Azure Blob Storage are similar to folders - they organize your blobs (files).\n",
    "Each storage account can contain unlimited containers, and each container can hold unlimited blobs.\n",
    "\n",
    "**Container naming rules:**\n",
    "- Must be lowercase\n",
    "- Can contain letters, numbers, and hyphens\n",
    "- Must start with a letter or number\n",
    "- Length: 3-63 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Define container name\n",
    "container_name = \"playground-data\"\n",
    "\n",
    "# Create container (if it doesn't exist)\n",
    "try:\n",
    "    container_client = blob_service_client.create_container(container_name)\n",
    "    print(f\"âœ“ Container '{container_name}' created successfully\")\n",
    "except Exception as e:\n",
    "    if \"ContainerAlreadyExists\" in str(e):\n",
    "        print(f\"â„¹ Container '{container_name}' already exists\")\n",
    "        container_client = blob_service_client.get_container_client(container_name)\n",
    "    else:\n",
    "        print(f\"âœ— Error creating container: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Checking Container Properties\n",
    "\n",
    "Once a container is created, we can inspect its properties and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Get container properties\n",
    "properties = container_client.get_container_properties()\n",
    "\n",
    "print(f\"\\nContainer Properties for '{container_name}':\")\n",
    "print(f\"  - Last Modified: {properties.last_modified}\")\n",
    "print(f\"  - ETag: {properties.etag}\")\n",
    "print(f\"  - Lease Status: {properties.lease.status}\")\n",
    "print(f\"  - Lease State: {properties.lease.state}\")\n",
    "\n",
    "# %%\n",
    "# List all containers in the storage account\n",
    "print(\"\\nAll containers in storage account:\")\n",
    "containers = blob_service_client.list_containers(include_metadata=True)\n",
    "for container in containers:\n",
    "    print(f\"  - {container.name}\")\n",
    "    if container.metadata:\n",
    "        print(f\"    Metadata: {container.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Adding Metadata to Containers\n",
    "\n",
    "Metadata is useful for storing additional information about containers, like descriptions, owners, or categories.\n",
    "Metadata consists of name-value pairs and is stored with the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Set metadata for the container\n",
    "metadata = {\n",
    "    'purpose': 'learning-project',\n",
    "    'environment': 'development',\n",
    "    'owner': 'rbrands'\n",
    "}\n",
    "\n",
    "container_client.set_container_metadata(metadata)\n",
    "print(f\"âœ“ Metadata added to container '{container_name}'\")\n",
    "\n",
    "# Retrieve and display metadata\n",
    "properties = container_client.get_container_properties()\n",
    "print(f\"\\nContainer Metadata:\")\n",
    "for key, value in properties.metadata.items():\n",
    "    print(f\"  - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 3. Uploading Blobs (Files)\n",
    "\n",
    "Now let's upload some files to our container. Azure Blob Storage supports three types of blobs:\n",
    "- **Block Blobs**: Optimized for uploading large amounts of data (text, images, videos, etc.)\n",
    "- **Append Blobs**: Optimized for append operations (log files)\n",
    "- **Page Blobs**: Optimized for random read/write operations (VHD files)\n",
    "\n",
    "We'll focus on **Block Blobs** as they're the most commonly used type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# First, let's create some sample data to upload\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a simple CSV file\n",
    "data = {\n",
    "    'id': range(1, 11),\n",
    "    'name': [f'Item_{i}' for i in range(1, 11)],\n",
    "    'value': np.random.randint(10, 100, 10),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 10)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Sample data created:\")\n",
    "print(df.head())\n",
    "\n",
    "# Save to local file (temporary)\n",
    "local_file_path = 'sample_data.csv'\n",
    "df.to_csv(local_file_path, index=False)\n",
    "print(f\"\\nâœ“ Sample file created: {local_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Uploading a File\n",
    "\n",
    "There are several ways to upload data to Azure Blob Storage:\n",
    "1. **From a local file** (most common)\n",
    "2. **From data in memory** (text, bytes)\n",
    "\n",
    "\n",
    "Let's try all methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Method 1: Upload from local file\n",
    "blob_name = \"data/sample_data.csv\"  # Note: we can use \"folders\" in the blob name\n",
    "\n",
    "# Get a blob client\n",
    "blob_client = container_client.get_blob_client(blob_name)\n",
    "\n",
    "# Upload the file\n",
    "with open(local_file_path, \"rb\") as data:\n",
    "    blob_client.upload_blob(data, overwrite=True)\n",
    "\n",
    "print(f\"âœ“ File uploaded to: {blob_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Method 2: Upload data directly from memory (without saving to disk first)\n",
    "text_data = \"\"\"\n",
    "This is a sample text file created in memory.\n",
    "It demonstrates uploading data without creating a local file first.\n",
    "This is useful for generated content or processed data.\n",
    "\"\"\"\n",
    "\n",
    "blob_name_text = \"data/sample_text.txt\"\n",
    "blob_client_text = container_client.get_blob_client(blob_name_text)\n",
    "blob_client_text.upload_blob(text_data, overwrite=True)\n",
    "\n",
    "print(f\"âœ“ Text uploaded to: {blob_name_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Method 3: Upload JSON data from memory\n",
    "import json\n",
    "\n",
    "json_data = {\n",
    "    'experiment': 'azure-blob-test',\n",
    "    'date': '2024-01-14',\n",
    "    'results': [\n",
    "        {'trial': 1, 'score': 0.85},\n",
    "        {'trial': 2, 'score': 0.91},\n",
    "        {'trial': 3, 'score': 0.88}\n",
    "    ]\n",
    "}\n",
    "\n",
    "blob_name_json = \"data/experiment_results.json\"\n",
    "blob_client_json = container_client.get_blob_client(blob_name_json)\n",
    "blob_client_json.upload_blob(json.dumps(json_data, indent=2), overwrite=True)\n",
    "\n",
    "print(f\"âœ“ JSON uploaded to: {blob_name_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Listing Blobs in a Container\n",
    "\n",
    "After uploading, let's verify that our files are in the container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# List all blobs in the container\n",
    "print(f\"\\nBlobs in container '{container_name}':\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "blob_list = container_client.list_blobs()\n",
    "for blob in blob_list:\n",
    "    print(f\"  ðŸ“„ {blob.name}\")\n",
    "    print(f\"     Size: {blob.size:,} bytes\")\n",
    "    print(f\"     Created: {blob.creation_time}\")\n",
    "    print(f\"     Last Modified: {blob.last_modified}\")\n",
    "    print()\n",
    "\n",
    "# %%\n",
    "# List blobs with a specific prefix (like a \"folder\")\n",
    "print(\"Blobs in 'data/' folder:\")\n",
    "blob_list = container_client.list_blobs(name_starts_with=\"data/\")\n",
    "for blob in blob_list:\n",
    "    print(f\"  - {blob.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Blob Properties and Metadata\n",
    "\n",
    "Just like containers, individual blobs can have properties and metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Get properties of a specific blob\n",
    "blob_client = container_client.get_blob_client(\"data/sample_data.csv\")\n",
    "properties = blob_client.get_blob_properties()\n",
    "\n",
    "print(f\"Properties for 'data/sample_data.csv':\")\n",
    "print(f\"  - Content Type: {properties.content_settings.content_type}\")\n",
    "print(f\"  - Size: {properties.size:,} bytes\")\n",
    "print(f\"  - ETag: {properties.etag}\")\n",
    "print(f\"  - Last Modified: {properties.last_modified}\")\n",
    "print(f\"  - Creation Time: {properties.creation_time}\")\n",
    "\n",
    "# %%\n",
    "# Set metadata for a blob\n",
    "metadata = {\n",
    "    'source': 'python-script',\n",
    "    'data_type': 'sample',\n",
    "    'version': '1.0'\n",
    "}\n",
    "\n",
    "blob_client.set_blob_metadata(metadata)\n",
    "print(\"\\nâœ“ Metadata added to blob\")\n",
    "\n",
    "# Retrieve metadata\n",
    "properties = blob_client.get_blob_properties()\n",
    "print(\"\\nBlob Metadata:\")\n",
    "for key, value in properties.metadata.items():\n",
    "    print(f\"  - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 4. Downloading Blobs (Files)\n",
    "\n",
    "Now that we have files in Azure Blob Storage, let's learn how to download them.\n",
    "Just like uploading, there are multiple ways to download data:\n",
    "1. **Download to a local file**\n",
    "2. **Download to memory** (for direct processing)\n",
    "3. **Stream download** (for large files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Method 1: Download blob to a local file\n",
    "download_file_path = \"downloaded_sample_data.csv\"\n",
    "blob_name = \"data/sample_data.csv\"\n",
    "\n",
    "blob_client = container_client.get_blob_client(blob_name)\n",
    "\n",
    "with open(download_file_path, \"wb\") as download_file:\n",
    "    download_file.write(blob_client.download_blob().readall())\n",
    "\n",
    "print(f\"âœ“ Blob downloaded to: {download_file_path}\")\n",
    "\n",
    "# Verify the download by reading the CSV\n",
    "df_downloaded = pd.read_csv(download_file_path)\n",
    "print(f\"\\nDownloaded data preview:\")\n",
    "print(df_downloaded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Downloading Directly to Memory\n",
    "\n",
    "For data processing workflows, it's often more efficient to load data directly into memory\n",
    "without creating intermediate files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Method 2: Download directly to memory and process\n",
    "blob_name = \"data/sample_data.csv\"\n",
    "blob_client = container_client.get_blob_client(blob_name)\n",
    "\n",
    "# Download as bytes\n",
    "blob_data = blob_client.download_blob().readall()\n",
    "\n",
    "# Convert bytes to pandas DataFrame directly\n",
    "from io import BytesIO\n",
    "df_from_blob = pd.read_csv(BytesIO(blob_data))\n",
    "\n",
    "print(\"Data loaded directly from blob:\")\n",
    "print(df_from_blob.head())\n",
    "print(f\"\\nShape: {df_from_blob.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Download text file to string\n",
    "blob_name = \"data/sample_text.txt\"\n",
    "blob_client = container_client.get_blob_client(blob_name)\n",
    "\n",
    "# Download as text\n",
    "text_content = blob_client.download_blob().readall().decode('utf-8')\n",
    "\n",
    "print(\"Text content from blob:\")\n",
    "print(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Download and parse JSON\n",
    "blob_name = \"data/experiment_results.json\"\n",
    "blob_client = container_client.get_blob_client(blob_name)\n",
    "\n",
    "# Download and parse JSON\n",
    "json_content = blob_client.download_blob().readall().decode('utf-8')\n",
    "experiment_data = json.loads(json_content)\n",
    "\n",
    "print(\"JSON data from blob:\")\n",
    "print(json.dumps(experiment_data, indent=2))\n",
    "\n",
    "# Access the data\n",
    "print(f\"\\nExperiment: {experiment_data['experiment']}\")\n",
    "print(f\"Number of trials: {len(experiment_data['results'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Streaming Large Files\n",
    "\n",
    "For large files, it's more memory-efficient to stream the download in chunks\n",
    "rather than loading everything into memory at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Method 3: Stream download (useful for large files)\n",
    "blob_name = \"data/sample_data.csv\"\n",
    "blob_client = container_client.get_blob_client(blob_name)\n",
    "\n",
    "download_stream = blob_client.download_blob()\n",
    "\n",
    "# Process in chunks\n",
    "chunk_size = 1024  # 1KB chunks\n",
    "chunks_processed = 0\n",
    "\n",
    "print(\"Streaming download:\")\n",
    "for chunk in download_stream.chunks():\n",
    "    chunks_processed += 1\n",
    "    # In a real scenario, you would process each chunk here\n",
    "    # For example: write to file, process data, etc.\n",
    "\n",
    "print(f\"âœ“ Downloaded in {chunks_processed} chunks\")\n",
    "print(f\"  Total size: {download_stream.size:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Downloading Multiple Files\n",
    "\n",
    "Often you need to download multiple files at once. Here's an efficient way to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Download all files from a \"folder\" (prefix)\n",
    "import os\n",
    "\n",
    "# Create a directory for downloads\n",
    "download_dir = \"downloads\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# List all blobs with prefix \"data/\"\n",
    "blob_list = container_client.list_blobs(name_starts_with=\"data/\")\n",
    "\n",
    "print(f\"Downloading all files from 'data/' folder:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for blob in blob_list:\n",
    "    # Create local file path\n",
    "    local_path = os.path.join(download_dir, blob.name.replace(\"data/\", \"\"))\n",
    "\n",
    "    # Download blob\n",
    "    blob_client = container_client.get_blob_client(blob.name)\n",
    "    with open(local_path, \"wb\") as download_file:\n",
    "        download_file.write(blob_client.download_blob().readall())\n",
    "\n",
    "    print(f\"âœ“ Downloaded: {blob.name} â†’ {local_path}\")\n",
    "\n",
    "print(f\"\\nâœ“ All files downloaded to '{download_dir}/' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Conditional Downloads\n",
    "\n",
    "You can download files only if they meet certain conditions (e.g., modified since last download):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Download only if blob was modified after a certain date\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "blob_name = \"data/sample_data.csv\"\n",
    "blob_client = container_client.get_blob_client(blob_name)\n",
    "\n",
    "# Get blob properties\n",
    "properties = blob_client.get_blob_properties()\n",
    "last_modified = properties.last_modified\n",
    "\n",
    "# Check if modified in the last hour (for example)\n",
    "one_hour_ago = datetime.now(last_modified.tzinfo) - timedelta(hours=1)\n",
    "\n",
    "if last_modified > one_hour_ago:\n",
    "    print(f\"âœ“ Blob was recently modified ({last_modified})\")\n",
    "    print(\"  Downloading latest version...\")\n",
    "    data = blob_client.download_blob().readall()\n",
    "    print(f\"  Downloaded {len(data):,} bytes\")\n",
    "else:\n",
    "    print(f\"â„¹ Blob not modified recently (last modified: {last_modified})\")\n",
    "    print(\"  Skipping download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## 5. Machine Learning Workflow with Azure Blob Storage\n",
    "\n",
    "Now let's see how Azure Blob Storage fits into a real machine learning workflow.\n",
    "We'll use the Wine Quality dataset and demonstrate:\n",
    "1. Loading data from Azure Blob Storage\n",
    "2. Training a model\n",
    "3. Saving the trained model to Azure Blob Storage\n",
    "4. Loading the model for predictions\n",
    "\n",
    "This simulates a typical ML workflow where:\n",
    "- Data scientists share datasets via cloud storage\n",
    "- Trained models are versioned and stored centrally\n",
    "- Models can be deployed from cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Import ML libraries\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pickle\n",
    "\n",
    "# Load the wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Create a DataFrame for better understanding\n",
    "wine_df = pd.DataFrame(X, columns=wine.feature_names)\n",
    "wine_df['target'] = y\n",
    "wine_df['target_name'] = wine_df['target'].map({0: wine.target_names[0],\n",
    "                                                   1: wine.target_names[1],\n",
    "                                                   2: wine.target_names[2]})\n",
    "\n",
    "print(\"Wine Dataset Overview:\")\n",
    "print(f\"  - Samples: {len(wine_df)}\")\n",
    "print(f\"  - Features: {len(wine.feature_names)}\")\n",
    "print(f\"  - Classes: {len(wine.target_names)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(wine_df['target_name'].value_counts())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(wine_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Step 1: Upload Dataset to Azure Blob Storage\n",
    "\n",
    "First, let's save and upload our dataset to Azure Blob Storage.\n",
    "This simulates a scenario where data scientists share datasets via cloud storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Save dataset to CSV\n",
    "dataset_filename = 'wine_dataset.csv'\n",
    "wine_df.to_csv(dataset_filename, index=False)\n",
    "\n",
    "# Upload to Azure Blob Storage in a 'datasets' folder\n",
    "blob_name = f\"ml-project/datasets/{dataset_filename}\"\n",
    "blob_client = container_client.get_blob_client(blob_name)\n",
    "\n",
    "with open(dataset_filename, \"rb\") as data:\n",
    "    blob_client.upload_blob(data, overwrite=True)\n",
    "\n",
    "print(f\"âœ“ Dataset uploaded to: {blob_name}\")\n",
    "\n",
    "# Add metadata\n",
    "metadata = {\n",
    "    'dataset': 'wine-quality',\n",
    "    'samples': str(len(wine_df)),\n",
    "    'features': str(len(wine.feature_names)),\n",
    "    'classes': str(len(wine.target_names)),\n",
    "    'uploaded_by': 'robert'\n",
    "}\n",
    "blob_client.set_blob_metadata(metadata)\n",
    "print(\"âœ“ Metadata added to dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Step 2: Load Dataset from Azure and Train Model\n",
    "\n",
    "Now let's load the dataset directly from Azure Blob Storage and train a model.\n",
    "This demonstrates how team members can access shared datasets without local copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Download dataset directly from Azure Blob Storage to memory\n",
    "blob_client = container_client.get_blob_client(blob_name)\n",
    "blob_data = blob_client.download_blob().readall()\n",
    "\n",
    "# Load into pandas DataFrame\n",
    "wine_df_from_azure = pd.read_csv(BytesIO(blob_data))\n",
    "\n",
    "print(\"âœ“ Dataset loaded from Azure Blob Storage\")\n",
    "print(f\"  Shape: {wine_df_from_azure.shape}\")\n",
    "\n",
    "# Prepare features and target\n",
    "X = wine_df_from_azure.drop(['target', 'target_name'], axis=1)\n",
    "y = wine_df_from_azure['target']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "\n",
    "# %%\n",
    "# Train a Random Forest model\n",
    "print(\"\\nTraining Random Forest model...\")\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"âœ“ Model trained successfully\")\n",
    "print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=wine.target_names))\n",
    "\n",
    "# %%\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "print(feature_importance.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Step 3: Save Trained Model to Azure Blob Storage\n",
    "\n",
    "After training, we save the model to Azure Blob Storage.\n",
    "This allows:\n",
    "- Model versioning\n",
    "- Team collaboration\n",
    "- Model deployment from a central location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Serialize the model using pickle\n",
    "model_filename = 'wine_classifier_model.pkl'\n",
    "\n",
    "# Save model locally first\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(f\"âœ“ Model serialized to: {model_filename}\")\n",
    "\n",
    "# Upload to Azure Blob Storage\n",
    "model_blob_name = f\"ml-project/models/{model_filename}\"\n",
    "blob_client = container_client.get_blob_client(model_blob_name)\n",
    "\n",
    "with open(model_filename, \"rb\") as data:\n",
    "    blob_client.upload_blob(data, overwrite=True)\n",
    "\n",
    "print(f\"âœ“ Model uploaded to: {model_blob_name}\")\n",
    "\n",
    "# Add metadata about the model\n",
    "model_metadata = {\n",
    "    'model_type': 'RandomForestClassifier',\n",
    "    'accuracy': str(round(accuracy, 4)),\n",
    "    'n_estimators': '100',\n",
    "    'train_samples': str(len(X_train)),\n",
    "    'test_samples': str(len(X_test)),\n",
    "    'features': str(len(X.columns)),\n",
    "    'trained_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "blob_client.set_blob_metadata(model_metadata)\n",
    "print(\"âœ“ Model metadata added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Step 4: Load Model from Azure for Predictions\n",
    "\n",
    "Now let's simulate a deployment scenario: loading the model from Azure Blob Storage\n",
    "to make predictions. This is how you would use models in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Download and load the model from Azure Blob Storage\n",
    "print(\"Loading model from Azure Blob Storage...\")\n",
    "blob_client = container_client.get_blob_client(model_blob_name)\n",
    "\n",
    "# Get model metadata first\n",
    "properties = blob_client.get_blob_properties()\n",
    "print(\"\\nModel Metadata:\")\n",
    "for key, value in properties.metadata.items():\n",
    "    print(f\"  - {key}: {value}\")\n",
    "\n",
    "# Download model\n",
    "model_data = blob_client.download_blob().readall()\n",
    "\n",
    "# Load model from bytes\n",
    "loaded_model = pickle.loads(model_data)\n",
    "print(\"\\nâœ“ Model loaded successfully from Azure\")\n",
    "\n",
    "# %%\n",
    "# Make predictions with the loaded model\n",
    "print(\"\\nTesting loaded model with new predictions:\")\n",
    "\n",
    "# Take a few samples from test set\n",
    "sample_indices = [0, 5, 10]\n",
    "X_sample = X_test.iloc[sample_indices]\n",
    "y_sample = y_test.iloc[sample_indices]\n",
    "\n",
    "# Predict\n",
    "predictions = loaded_model.predict(X_sample)\n",
    "probabilities = loaded_model.predict_proba(X_sample)\n",
    "\n",
    "# Display results\n",
    "for i, (idx, pred, prob, true) in enumerate(zip(sample_indices, predictions, probabilities, y_sample)):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  True class: {wine.target_names[true]}\")\n",
    "    print(f\"  Predicted class: {wine.target_names[pred]}\")\n",
    "    print(f\"  Confidence: {prob[pred]:.2%}\")\n",
    "    print(f\"  All probabilities: {dict(zip(wine.target_names, [f'{p:.2%}' for p in prob]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### Step 5: Model Versioning Example\n",
    "\n",
    "In real projects, you'll train multiple versions of your model.\n",
    "Let's see how to implement simple versioning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Create a versioned model name\n",
    "from datetime import datetime\n",
    "\n",
    "version = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "versioned_model_name = f\"ml-project/models/wine_classifier_v{version}.pkl\"\n",
    "\n",
    "# Upload with version in name\n",
    "blob_client = container_client.get_blob_client(versioned_model_name)\n",
    "with open(model_filename, \"rb\") as data:\n",
    "    blob_client.upload_blob(data, overwrite=True)\n",
    "\n",
    "# Add comprehensive metadata\n",
    "versioned_metadata = {\n",
    "    'model_type': 'RandomForestClassifier',\n",
    "    'version': version,\n",
    "    'accuracy': str(round(accuracy, 4)),\n",
    "    'dataset': 'wine-quality',\n",
    "    'n_estimators': '100',\n",
    "    'random_state': '42',\n",
    "    'description': 'Production model with optimized hyperparameters'\n",
    "}\n",
    "\n",
    "blob_client.set_blob_metadata(versioned_metadata)\n",
    "print(f\"âœ“ Versioned model saved: {versioned_model_name}\")\n",
    "\n",
    "# %%\n",
    "# List all model versions\n",
    "print(\"\\nAll model versions in storage:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "model_blobs = container_client.list_blobs(name_starts_with=\"ml-project/models/\")\n",
    "for blob in model_blobs:\n",
    "    print(f\"\\nðŸ“¦ {blob.name}\")\n",
    "    print(f\"   Size: {blob.size:,} bytes\")\n",
    "    print(f\"   Created: {blob.creation_time}\")\n",
    "\n",
    "    # Get metadata if available\n",
    "    blob_client = container_client.get_blob_client(blob.name)\n",
    "    properties = blob_client.get_blob_properties()\n",
    "    if properties.metadata:\n",
    "        print(\"   Metadata:\")\n",
    "        for key, value in properties.metadata.items():\n",
    "            print(f\"     - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Summary: Complete ML Workflow\n",
    "\n",
    "We've demonstrated a complete machine learning workflow using Azure Blob Storage:\n",
    "\n",
    "1. âœ“ **Data Storage**: Uploaded dataset to cloud storage with metadata\n",
    "2. âœ“ **Data Loading**: Loaded data directly from Azure into memory for training\n",
    "3. âœ“ **Model Training**: Trained a Random Forest classifier\n",
    "4. âœ“ **Model Storage**: Saved trained model to Azure with metadata\n",
    "5. âœ“ **Model Deployment**: Loaded model from Azure for predictions\n",
    "6. âœ“ **Versioning**: Implemented simple model versioning strategy\n",
    "\n",
    "**Benefits of this approach:**\n",
    "- **Collaboration**: Team members can access shared datasets and models\n",
    "- **Reproducibility**: Models and data are versioned and centrally stored\n",
    "- **Scalability**: No local storage limitations\n",
    "- **Deployment**: Models can be loaded from anywhere with internet access\n",
    "- **Tracking**: Metadata helps track model performance and lineage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
